"""
Model loading utilities for the Prompt Engineering Studio.
Handles loading and caching of Hugging Face models and prompt engineering tools.
"""

import streamlit as st
from transformers import pipeline
import torch
from typing import Optional, Any
import logging
from models.fake_llm import fake_llm
from models.prompt_engineering_tools import (
    prompt_refiner, 
    prompt_analyzer, 
    few_shot_generator, 
    cot_builder
)

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@st.cache_resource
def load_model(model_name: str) -> Optional[Any]:
    """
    Load a Hugging Face model or prompt engineering tool.

    Args:
        model_name (str): The model name/path or tool identifier

    Returns:
        Pipeline, function, or None: The loaded model/tool or None if loading fails
    """
    try:
        logger.info(f"Loading model/tool: {model_name}")

        # Handle prompt engineering tools
        if model_name == "prompt_refiner":
            return prompt_refiner
        elif model_name == "prompt_analyzer":
            return prompt_analyzer
        elif model_name == "few_shot_generator":
            return few_shot_generator
        elif model_name == "cot_builder":
            return cot_builder
        elif model_name.lower() == "fakegpt":
            return fake_llm

        # Determine the task based on the model
        # All supported models use text-generation task
        task = "text-generation"

        # Load the model with CPU-only settings
        model_pipeline = pipeline(
            task=task,
            model=model_name,
            device=-1,  # CPU only
            torch_dtype=torch.float32,  # Use float32 for better CPU compatibility
            trust_remote_code=False,  # Security best practice
            return_full_text=False if task == "text-generation" else True,
            max_new_tokens=50,  # Default limit for memory efficiency
            model_kwargs={"low_cpu_mem_usage": True},  # Optimize for low memory usage
        )

        logger.info(f"Successfully loaded model: {model_name}")
        return model_pipeline

    except Exception as e:
        logger.error(f"Failed to load model {model_name}: {str(e)}")
        st.error(f"❌ Failed to load model '{model_name}': {str(e)}")
        return None


def generate_text(model_pipeline: Any, prompt: str, max_new_tokens: int = 50) -> str:
    """
    Generate text using the loaded model pipeline.

    Args:
        model_pipeline: The loaded transformers pipeline
        prompt (str): The input prompt text
        max_new_tokens (int): Maximum number of new tokens to generate

    Returns:
        str: The generated text or error message
    """
    try:
        if model_pipeline is None:
            return "❌ Model not loaded. Please try selecting a different model."

        # Generate text with the pipeline
        result = model_pipeline(
            prompt,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=model_pipeline.tokenizer.eos_token_id,
        )

        # Extract the generated text based on pipeline type
        if isinstance(result, list) and len(result) > 0:
            if "generated_text" in result[0]:
                generated = result[0]["generated_text"]
                # For text-generation, remove the original prompt if it's included
                if generated.startswith(prompt):
                    generated = generated[len(prompt) :].strip()
                return generated
            elif "summary_text" in result[0]:
                return result[0]["summary_text"]
            else:
                return str(result[0])
        else:
            return "❌ No output generated by the model."

    except Exception as e:
        logger.error(f"Text generation failed: {str(e)}")
        return f"❌ Generation failed: {str(e)}"


def get_model_info(model_name: str) -> dict:
    """
    Get information about the model or tool for display purposes.

    Args:
        model_name (str): The model name or tool identifier

    Returns:
        dict: Model/tool information
    """
    model_info = {
        "prompt_refiner": {
            "type": "AI Prompt Optimizer",
            "size": "~0MB",
            "task": "prompt-optimization",
            "description": "Professional prompt refinement and optimization tool",
        },
        "prompt_analyzer": {
            "type": "Prompt Structure Analyzer",
            "size": "~0MB", 
            "task": "prompt-analysis",
            "description": "Analyzes prompt structure and provides improvement suggestions",
        },
        "few_shot_generator": {
            "type": "Few-Shot Example Generator",
            "size": "~0MB",
            "task": "example-generation", 
            "description": "Creates few-shot examples for better prompt engineering",
        },
        "cot_builder": {
            "type": "Chain-of-Thought Builder",
            "size": "~0MB",
            "task": "reasoning-enhancement",
            "description": "Builds chain-of-thought prompts for improved reasoning",
        },
        "fakegpt": {
            "type": "Legacy Prompt Refiner",
            "size": "~0MB",
            "task": "prompt-refinement",
            "description": "Basic prompt refinement (legacy mode)",
        },
        "google/flan-t5-small": {
            "type": "T5 Validation Model",
            "size": "~80MB",
            "task": "text-generation",
            "description": "Instruction-tuned model for testing refined prompts",
        },
        "distilgpt2": {
            "type": "GPT-2 Baseline",
            "size": "~353MB", 
            "task": "text-generation",
            "description": "Baseline model for comparison testing",
        },
        "sshleifer/tiny-gpt2": {
            "type": "GPT-2 (Tiny)",
            "size": "~40MB",
            "task": "text-generation",
            "description": "Ultra-lightweight GPT-2 variant for testing",
        },
        "gpt2": {
            "type": "GPT-2 Base",
            "size": "~548MB",
            "task": "text-generation",
            "description": "Standard GPT-2 model for text generation",
        },
        "microsoft/DialoGPT-small": {
            "type": "DialoGPT Small",
            "size": "~353MB",
            "task": "text-generation",
            "description": "Conversational AI model optimized for dialogue generation",
        },
    }

    return model_info.get(
        model_name,
        {
            "type": "Unknown",
            "size": "Unknown",
            "task": "text-generation",
            "description": "Custom model or tool",
        },
    )

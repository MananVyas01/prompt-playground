"""
Model loading utilities for the Prompt Playground app.
Handles loading and caching of Hugging Face models optimized for CPU inference.
"""

import streamlit as st
from transformers import pipeline
import torch
from typing import Optional, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@st.cache_resource
def load_model(model_name: str) -> Optional[Any]:
    """
    Load a Hugging Face model using transformers pipeline.
    
    Args:
        model_name (str): The model name/path from Hugging Face Hub
        
    Returns:
        transformers.Pipeline or None: The loaded pipeline or None if loading fails
    """
    try:
        logger.info(f"Loading model: {model_name}")
        
        # Determine the task based on the model
        if "t5" in model_name.lower() or "flan" in model_name.lower():
            task = "text2text-generation"
        else:
            task = "text-generation"
        
        # Load the model with CPU-only settings
        model_pipeline = pipeline(
            task=task,
            model=model_name,
            device=-1,  # CPU only
            torch_dtype=torch.float32,  # Use float32 for better CPU compatibility
            trust_remote_code=False,  # Security best practice
            return_full_text=False if task == "text-generation" else True,
            max_new_tokens=50,  # Default limit for memory efficiency
            model_kwargs={"low_cpu_mem_usage": True}  # Optimize for low memory usage
        )
        
        logger.info(f"Successfully loaded model: {model_name}")
        return model_pipeline
        
    except Exception as e:
        logger.error(f"Failed to load model {model_name}: {str(e)}")
        st.error(f"❌ Failed to load model '{model_name}': {str(e)}")
        return None

def generate_text(model_pipeline: Any, prompt: str, max_new_tokens: int = 50) -> str:
    """
    Generate text using the loaded model pipeline.
    
    Args:
        model_pipeline: The loaded transformers pipeline
        prompt (str): The input prompt text
        max_new_tokens (int): Maximum number of new tokens to generate
        
    Returns:
        str: The generated text or error message
    """
    try:
        if model_pipeline is None:
            return "❌ Model not loaded. Please try selecting a different model."
        
        # Generate text with the pipeline
        result = model_pipeline(
            prompt,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=model_pipeline.tokenizer.eos_token_id,
        )
        
        # Extract the generated text based on pipeline type
        if isinstance(result, list) and len(result) > 0:
            if 'generated_text' in result[0]:
                generated = result[0]['generated_text']
                # For text-generation, remove the original prompt if it's included
                if generated.startswith(prompt):
                    generated = generated[len(prompt):].strip()
                return generated
            elif 'summary_text' in result[0]:
                return result[0]['summary_text']
            else:
                return str(result[0])
        else:
            return "❌ No output generated by the model."
            
    except Exception as e:
        logger.error(f"Text generation failed: {str(e)}")
        return f"❌ Generation failed: {str(e)}"

def get_model_info(model_name: str) -> dict:
    """
    Get information about the model for display purposes.
    
    Args:
        model_name (str): The model name
        
    Returns:
        dict: Model information
    """
    model_info = {
        "sshleifer/tiny-gpt2": {
            "type": "GPT-2 (Tiny)",
            "size": "~40MB",
            "task": "text-generation",
            "description": "Ultra-lightweight GPT-2 variant for testing"
        },
        "distilgpt2": {
            "type": "DistilGPT-2", 
            "size": "~353MB",
            "task": "text-generation",
            "description": "Distilled version of GPT-2, 2x faster, same performance"
        },
        "google/flan-t5-small": {
            "type": "FLAN-T5 Small",
            "size": "~308MB", 
            "task": "text2text-generation",
            "description": "T5 model fine-tuned for instruction following"
        }
    }
    
    return model_info.get(model_name, {
        "type": "Unknown",
        "size": "Unknown",
        "task": "text-generation",
        "description": "Custom model"
    })
